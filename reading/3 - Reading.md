# **Introduction Non-Linear Optimization**
## **1. Classic Economic Dispatch (CED)**
As already introduced economic dispatch problem consists in allocating the total demand among generating units so that the production cost is minimized.
Each generating unit is assigned a function, $C_i(P_{Gi})$, characterizing its generating cost in €/h in terms of the power produced in MW, $P_{Gi}$, during 1 h. This function is obtained by multiplying the heat rate curve, expressing the fuel consumed to produce 1MW during 1 h, by the cost of the fuel consumed during that hour. Note that the heat rate is a measure of the energy efficiency of the generating unit.
The cost function is generally approximated by a convex quadratic or piecewise linear function, as illustrated in Figure 1.
Considering $n$ generating units, the total production cost is
$$ C(P_G) = \sum_{i=1}^n C_i(P_{Gi}) $$

where $P_G$ is the column vector of the unit generation levels $P_{Gi}$.

# <center> ![image.png](/images/3Reading_fig1.png)
<!-- <img src="/images/3Reading_fig1.png" width="300" height="300" /> -->


_**Figure 1**. Example of the quadratic cost function $C_i(P_{Gi})$ of unit “i”_

We first consider the basic CED without losses ($P_{loss} =0$) and without generation limits; this results in a quadratic optimization problem, which can be seen as a special case of a non-linear optimization problem.

## **2. Nonlinear Optimization without Constraints (Non-linear Programming NLP)**

Suppose $f(\bm{x})$ is a scalar function of $n$ variables ($x_1, x_2, ..., x_n$), which represent degrees of freedom (optimization variables): $ f(\bm{x}): \ \mathbb{R}^n \rightarrow \mathbb{R}$

If the function $f$ is twice differentiable, then we will say: $ f \in  C^2(\mathbb{R}^n \rightarrow \mathbb{R})$.
The easiest way to develop the necessary and sufficient conditions for the optimality of $f(\bm{x})$ is to start with a Taylor series expansion about the presumed extremum $\bm{x}^*$.

$$ f(\bm{x}) = f(\bm{x}^*) + \nabla^T f(\bm{x}^*)\Delta \bm{x} + \frac{1}{2}(\Delta \bm{x}^T) \nabla^2 f(\bm{x}^*)(\Delta \bm{x}) + O_3(\Delta \bm{x}) \ \ \ \tag{1} $$

where $\bm{x} - \bm{x}^* = \Delta \bm{x}$ is the small perturbation of $\bm{x}$ from $\bm{x}^*$ and $\nabla f(\bm{x})$ is the gradient of the function $f(\bm{x})$. \
We assume that all terms in the above mentioned equation exist and are continuous, but we will ignore the terms of order 3 or higher $O_3(\Delta \bm{x})$.

We defined a local minimum as a point $\bm{x}^*$ such no other points in the vicinity of $\bm{x}$ yields a value of $f(\bm{x})$ less than $f(\bm{x}^*)$:
$$ f(\bm{x}) - f(\bm{x}^*) \geq 0 \ \ \ \tag{2}$$

We will say that $\bm{x}^*$ is a *global* minimum if this equation holds for any $\bm{x}$ in the n-dimensional space of $\bm{x}$. \
Similarly, $\bm{x}^*$ is a *local* maximum if:
$$ f(\bm{x}) - f(\bm{x}^*) \leq 0 \ \ \ \tag{3}$$

### **Gradient of a function f(x)**

The first partial derivative of f(\bm{x}) is called a gradient and is denoted $\nabla f(\bm{x})$:

$$ grad f(\bm{x}) = \nabla f(\bm{x}) = \bigg [ \frac{\partial f}{\partial x_1 } \frac{\partial f}{\partial x_2 } ... \frac{\partial f}{\partial x_n } \bigg ]^T \ \ \ \tag{4} $$

The gradient is a column vector.

# <center> ![Figure 2:. The gradient of the function $f(x,y) = −(cos2x + cos2y)^2$ depicted as a vector field on the bottom plan [plot generated by P.A. Simionescu http://en.wikipedia.org/wiki/File:Gradient99.png]](/images/3Reading_fig2.png)

_**Figure 2**:. The gradient of the function $f(x,y) = −(cos2x + cos2y)^2$ depicted as a vector field on the bottom plan [plot generated by P.A. Simionescu http://en.wikipedia.org/wiki/File:Gradient99.png]_
### Hessian of the function f(x)
The function $\nabla^2 f(\bm{x})$ is called the Hessian matrix and is defined as follows:

$$
H(f) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$
The hessian matrix is symmetric for continuously differentiable functions.

### **2.1. Necessary optimality condition**
Examine the second term on the right-hand side of (1): $\Delta \bm{x}^T \nabla^2 f(\bm{x}^*)\Delta \bm{x}$. Because $\Delta \bm{x}$ is arbitrary and can have both plus and minus values for its elements, we must insist that $\bm{x}^*$ is a stationary point. Hence, a necessary condition for a minimum or maximum of $f(\bm{x})$ is that the gradient of $f(\bm{x})$ vanishes at $\bm{x}^*$.

$$ \nabla f(\bm{x}^*) = grad f(\bm{x}^*) = 0 \ \  \tag{5}$$

$\bm{x}^*$ is called a stationary point. A stationary point is a candidate for a local optimum; to become a local optimum the stationary point should satisfy the sufficient conditions for the optimality.

### **2.2. Sufficient optimality conditions**
Satisfaction of the necessary conditions does not guarantee an optimum. To define sufficient conditions we will analyze the third term of (1): $\Delta \bm{x}^T \nabla^2 f(\bm{x}^*)\Delta \bm{x}$. This term establishes the character of the stationary point (minimum, maximum or saddle point). \
The sufficient conditions for the optimality require that $H(\bm{x}^*)$ is positive-definite for a minimum to exist ar $\bm{x^*}$ and negative-definite for a maximum to exist at $\bm{x}^*$. 
To determine if $H(\bm{x}^*)$ is positive-definite the eigenvalues $\lambda$ of the matrix $H(\bm{x})$ should be calculated by using the determinant of $(\bm{H}(\bm{x} - \lambda \bm{I}))$.

If $\text{det}(\bm{H}(\bm{x} - \lambda \bm{I})) = 0$ and all the eigenvalues $\lambda$ are positive, then $H(\bm{x}^*)$ is positive-definite and the function $f(\bm{x})$ is strictly convex (has a minimum). If all the eigenvalues $\lambda$ are positive or equal to zero, then $H(\bm{x}^*)$ is positive-semidefinite. If all the eigenvalues $\lambda$ are negative, then $H(\bm{x}^*)$ is negative-definite and the function $f(\bm{x})$ is strictly concave (has a maximum).

### **2.3 Example analystical calculation optimum**
Suppose that the following function of two variables $x_1$ and $x_2$ should be optimized and no constraints should be taken into account:
$$ f(\bm{x}) = 2x_1^2 -3x_1x_2 + 2x_2^2 $$
$$ grad f(\bm{x}) = \bigg [ 4x_1 -3x_2, -3x_1 + 4x_2 \bigg]^T$$

The stationary point satisfies: $4x_1 -3x_2 = 0$ and $-3x_1 + 4x_2 = 0$. The stationary point is in $(0, 0)$.
The Hessian matrix is:
$$ H(\bm{x}) = \begin{bmatrix} 4 & -3 \\ -3 & 4 \end{bmatrix}$$

The eigenvalues of the Hessian matrix are:
$$ \text{det}(H(\bm{x}) -\lambda \bm{I}) = \begin{bmatrix} 4-\lambda & -3 \\ -3 & 4-\lambda \end{bmatrix} = 0$$
$$(4-\lambda)^2 - (-3)^2 = 0$$
$ \lambda_1 = 1$ and $\lambda_2 = 7$. Both eigenvalues are positive, so the stationary point is a minimum.

## **3. Nonlinear Optimization with Constraints**
The above mentioned necessary and sufficient conditions for optimality of non-linear functions do not apply for the situations with constraints. When the optimization problem contains constraints:

$$ \underset{(x_1, ...,x_n)}{\text{min}} \ f(\bm{x}): \mathbb{R}^n \rightarrow \mathbb{R} $$

subject to:
$$ h_i(\bm{x}) = b_i \ \  i=1,2,...,m \ \  \textbf{(equality constraints)} $$
$$ g_j(\bm{x}) \leq c_j \ \  j=1,2,...,r \ \  \textbf{(inequality constraints)} $$
other necessary conditions for optimum apply.

### **3.1. Example analytical calculation optimum for problems containing only equality constraints**

In case only one equality constraint is given, a new function, the so-called Lagrangian function with a Lagrange multiplier $\lambda$ should be introduced:
$$ L(\bm{x},\lambda) = f(\bm{x}) + \lambda h(\bm{x})$$

The necessary conditions for the optimality for a constrained problem with equality constraints are:
Let $\bm{x}^*$ be a local minimum (or maximum) for the problem
$$ \underset{(x_1, ...,x_n)}{\text{min}} \ f(\bm{x}): \mathbb{R}^n \rightarrow \mathbb{R} $$
subject to:
$$ h_i(\bm{x}) = b_i \ \  i=1,2,...,m \ \  \textbf{(equality constraints)} $$
and assume that the constraint gradients, $i=1,2,...,m$ are linearly independent. Then there exists a vector of Langrange multipliers $\bm{\lambda}^*=(\lambda_1, \lambda_2,..., \lambda_m)$ such that $(\bm{x}^*, \bm{\lambda}^*)$ satisfies the conditions:

$$ \frac{\partial L(\bm{x},\bm{\lambda})}{\partial \bm{x}}= 0 \ \ \ \text{and} \ \ \  \frac{\partial L(\bm{x},\bm{\lambda})}{\partial \bm{\lambda}}= 0 $$

The Lagrange multipliers $\bm{\lambda}^*$ provide useful information on right-hand side changes of a constraint, just as they do for linear programs. The constraints with the largest absolute $\lambda_j$ are the ones whose right-hand sides affect the optimal value of the objective function the most.

Suppose that the following function of two variables $x_1$ and $x_2$ should be optimized subject to one equality constraint:
$$ f(\bm{x}) = 4x_1^2 + 5x_2^2 $$
Subject to the equality constraint $h(x)$:
$$2 x_1 + 3 x_2 - 6 = 0$$
The Lagrange function:
$$ L(\bm{x}, \lambda) = 4x_1^2 + 5x_2^2 + \lambda (2x_1 + 3x_2 - 6)$$

Applying the necessary conditions for the optimality gives:
$$ \frac{\partial L(\bm{x}, \lambda)}{\partial x_1} = 8x_1 + 2\lambda = 0 \ \ \ \text{and} \ \ \ \frac{\partial L(\bm{x}, \lambda)}{\partial x_2} = 10x_2 + 3\lambda = 0$$
$$ \frac{\partial L(\bm{x}, \lambda)}{\partial \lambda} = 2x_1 + 3x_2 - 6 = 0$$
which results in:
$$ \lambda^* = -4.286$$
$$ x_1^* = 1.071$$
$$ x_2^* = 1.286$$

### **3.2. Example analytical calculation optimum for problems containing equality and inequality constraints**

When both equality and inequality constraints are present the Lagrangian function with Lagrange multipliers $\lambda$ associated with the equality constraints and multipliers $u$ for the inequality constraints, has the following form (Kuhn-Tucker KKT conditions):
$$ L(x,\lambda,\mu) = f(x) + \sum_{i=1}^m \lambda_i [h_i(x)-b_i] + \sum_{j=1}^r \mu_j [g_j(x)-c_j]$$

Then there exists a vector of Langrange multipliers $\bm{\lambda}^*=(\lambda_1, \lambda_2,..., \lambda_m)$ and KTT multipliers $\bm{\mu}^*=(\mu_1,..., \mu_r)$ such that $(\bm{x}^*, \bm{\lambda}^*, \bm{\mu}^*)$ satisfies the conditions:
$$ \frac{\partial L(\bm{x},\bm{\lambda},\bm{\mu})}{\partial \bm{x}}= 0 \ \ \ \text{and} \ \ \  \frac{\partial L(\bm{x},\bm{\lambda},\bm{\mu})}{\partial \bm{\lambda}}= 0 $$
and
$$ \mu_j^* \geq 0 \ \ \ \text{and} \ \ \  \mu_j^* [g_j(\bm{x}^*)-c_j] = 0 $$
-----
***Example***

Suppose that the following function of two variables $x_1$ and $x_2$ should be minimized
$$ f(\bm{x}) = x_1 x_2 $$
Subject to the inequality constraint $g(\bm{x})$:
$$g(\bm{x}) = x_1^2 + x_2^2 \leq 25$$
The Lagrange function is:
$$ L(\bm{x}, \mu) = x_1 x_2 + \mu (x_1^2 + x_2^2 - 25)$$
The necessary conditions for a stationary point are:
$$ \frac{\partial L(\bm{x}, \mu)}{\partial x_1} = x_2 + 2\mu x_1 = 0 \ \ \ \text{and} \ \ \ \frac{\partial L(\bm{x}, \mu)}{\partial x_2} = x_1 + 2\mu x_2 = 0$$
$$ \frac{\partial L(\bm{x}, \mu)}{\partial \mu} = x_1^2 + x_2^2 - 25 = 0$$
$$\mu \geq 0 \ \  \text{and} \ \ \mu ( 25 - x_1^2 - x_2^2) = 0$$
When searching for a maximum the last condition should be replaced by $\mu \leq 0$. If $\mu = 0$ the the solution is a saddle point.

The solution of the problem is given in the table below:

| $\mu$ 	| $x_1$ 	| $x_2$ 	| **Point** 	| **Constraint** 	| **Optimum**   	|
|-------	|-------	|-------	|-----------:	|----------------	|---------------	|
| 0     	| 0     	| 0     	| A         	| =25            	| = 0 (saddle)   	|
| 0,5   	| 3,54  	| -3,54 	| B         	| =0             	| = -12,5 (min) 	|
| 0,5   	| -3,54 	| 3,54  	| C         	| =0             	| = -12,5 (min) 	|
| -0,5  	| 3,54  	| 3,54  	| D         	| =0             	| = +12,5 (max) 	|
| -0,5  	| -3,54 	| -3,54 	| E         	| =0             	| = +12,5 (max) 	|

# <center> ![Figure 3. The optimum of the function $f(\bm{x})=x_1x_2$](/images/3Reading_fig3.png)

***Figure 3***. *The optimum of the function $f(\bm{x})=x_1x_2$*

The necessary conditions are satisfied at any local minimum, maximum or saddle point. To guarantee the optimality the sufficient conditions should be satisfied, which involve the matrix of second partial derivatives with respect to x (the Hessian matrix of the Lagrange function). How to use this sufficient condition see e.g. T.F. Edgar and
D.M. Himmelblauw, Optimization of chemical processes, McGrawHill, Boston, 2001.




